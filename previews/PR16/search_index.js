var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Standard-usage","page":"API","title":"Standard usage","text":"","category":"section"},{"location":"api/#Newton.newtonsolve","page":"API","title":"Newton.newtonsolve","text":"newtonsolve(rf!, x0::AbstractVector, [cache::NewtonCache]; tol=1.e-6, maxiter=100)\n\nSolve the nonlinear equation (system) r(x)=0 using the newton-raphson method by calling the mutating residual function rf!(r, x), with signature rf!(r::T, x::T)::T where T<:AbstractVector x0 is the initial guess and the optional cache can be preallocated by calling NewtonCache(x0). Note that x0 is not modified, unless aliased to getx(cache).  tol is the tolerance for norm(r) and maxiter the maximum number of iterations. \n\nreturns x, drdx, converged::Bool\n\ndrdx is the derivative of r wrt. x at the returned x.\n\n\n\n\n\nnewtonsolve(rf, x0::T; tol=1.e-6, maxiter=100, [linsolver]) where {T <: \n    Union{Number, StaticArrays.SVector, Tensors.Vec, Tensors.SecondOrderTensor}}\n\nSolve the nonlinear equation (system) r(x)=0 using the newton-raphson method by calling the residual function r=rf(x), with signature rf(x::T)::T x0::T is the initial guess, tol the tolerance form norm(r), and maxiter the maximum number  of iterations.\n\nA non-standard linsolver can optionally be specified, please see Linear solvers for more information.\n\nreturns: x, drdx, converged::Bool\n\ndrdx is the derivative of r wrt. x at the returned x.\n\n\n\n\n\n","category":"function"},{"location":"api/#Newton.NewtonCache","page":"API","title":"Newton.NewtonCache","text":"function NewtonCache(x::AbstractVector; [linsolver])\n\nCreate the cache used by the newtonsolve and linsolve!.  Only a copy of x will be used.\n\nA special linsolver can optionally be given, please see Linear solvers for more information.\n\n\n\n\n\n","category":"type"},{"location":"api/#Newton.getx","page":"API","title":"Newton.getx","text":"getx(cache::NewtonCache)\n\nExtract out the unknown values. This can be used to avoid  allocations when solving defining the initial guess. \n\n\n\n\n\n","category":"function"},{"location":"api/#Newton.logging_mode","page":"API","title":"Newton.logging_mode","text":"Newton.logging_mode(; enable=true)\n\nHelper to turn on (enable=true) or off (enable=false) logging of iterations in Newton.jl. Internally, changes the how Newton.@if_logging expr is evaluated:  when logging mode is enabled, expr is evaluated, otherwise expr is ignored.\n\n\n\n\n\n","category":"function"},{"location":"api/#Available-linear-solvers","page":"API","title":"Available linear solvers","text":"","category":"section"},{"location":"api/#Newton.AbstractLinsolver","page":"API","title":"Newton.AbstractLinsolver","text":"Newton.jl comes with the following linear solvers\n\nStandardLinsolver (default)\nUnsafeFastLinsolver\nRecursiveFactorizationLinsolver\n\n\n\n\n\n","category":"type"},{"location":"api/#Newton.StandardLinsolver","page":"API","title":"Newton.StandardLinsolver","text":"Newton.StandardLinsolver()\n\nThis is the default linear solver, which gives safe operations and don't require any special packages to be loaded.\n\n\n\n\n\n","category":"type"},{"location":"api/#Newton.UnsafeFastLinsolver","page":"API","title":"Newton.UnsafeFastLinsolver","text":"UnsafeFastLinsolver()\n\nThis is a special linear solver, which calculates the inverse recursively by using the analytical inverses of 2x2 and 3x3 matrices. This gives exceptional performance for small matrices, but suffers from numerical errors and can be sensitive to  badly conditioned matrices. When using this method, it may be advisable to (adaptively) try a slower method if the newton  iterations fail to converge.\n\n\n\n\n\n","category":"type"},{"location":"api/#Newton.RecursiveFactorizationLinsolver","page":"API","title":"Newton.RecursiveFactorizationLinsolver","text":"RecursiveFactorizationLinsolver()\n\nThis linear solver utilizes the LU decomposition in RecursiveFactorization.jl, which gives faster performance than the  StandardLinsolver. While not as fast as UnsafeFastLinsolver, it is always accurate. Is available via an extension,  requiring the user to load RecursiveFactorization.jl separately. \n\n\n\n\n\n","category":"type"},{"location":"api/#Fast-inverse","page":"API","title":"Fast inverse","text":"","category":"section"},{"location":"api/#Newton.inv!","page":"API","title":"Newton.inv!","text":"Newton.inv!(A::Matrix, cache::NewtonCache)\n\nIn-place inverse, which, depending on the linsolver in cache, can be much more  efficient than inv(A). However, note that A will be used as workspace and values should not be used after calling Newton.inv!. In some cases,  A will become its inverse, and the output aliased to A.  This behavior is not true in general, and should not be relied upon. \n\n\n\n\n\n","category":"function"},{"location":"api/#Newton.sinv","page":"API","title":"Newton.sinv","text":"sinv(a::SMatrix{d, d}) where {d}\n\nFast, but numerically unstable implementation of the inverse of a statically sized matrix. Beneficial up to d ≈ 50, but can give large floating point errors for badly conditioned and/or large matrices.\n\nAbout 4 to 5 timers faster than StaticArrays for sizes ∈ [5, 20], and at least twice as fast up to 50 according  to benchmarks on macbook with M3 processor.\n\n\n\n\n\n","category":"function"},{"location":"api/#Newton.sinv!","page":"API","title":"Newton.sinv!","text":"sinv!(K::Matrix)\n\nInvert K in-place using the unsafe static implementation up to a size of 20x20, and fall back to generic LinearAlgebra.inv\n\n\n\n\n\n","category":"function"},{"location":"api/#Use-inside-AD-calls","page":"API","title":"Use inside AD-calls","text":"","category":"section"},{"location":"api/#Newton.ad_newtonsolve","page":"API","title":"Newton.ad_newtonsolve","text":"ad_newtonsolve(rf, x0, rf_dual_args::Tuple; kwargs...)\n\nSolve rf(x, y₁(z), y₂(z), ...) = 0 to find x(y₁(z), y₂(z), ...), given the initial guess x0 as a non-dual number, and the dual input numbers  rf_dual_args = (y₁(z), y₂(z), ...) where all are derivatives wrt. the same variable z. Return x of Dual type seeded such that it corresponds to the derivative dx/dz = ∂x/∂yᵢ ⋆ dyᵢ/dz where ⋆ is the appropriate contraction.\n\nImplementation: \n\nUses the adjoint, i.e. dr/dyᵢ = 0 = ∂r/∂x ⋆ ∂x/∂yᵢ + ∂r/∂yᵢ ⇒ ∂x/∂yᵢ = -[∂r/∂x]⁻¹ ⋆ ∂r/∂yᵢ,  such that we avoid doing newton iterations with dual numbers. \n\nad_newtonsolve(rf, x0, rf_args::Tuple; kwargs...)\n\nIf rf_args do not contain dual numbers, the standard newtonsolver is just called on  f(x) = rf(x, y₁, y₂, ...), and the solution x is returned. This allows writing generic  code where the gradient is sometimes required, but not always. \n\nExample\n\nusing Newton, Tensors, ForwardDiff, BenchmarkTools\nrf(x::Vec, a::Number) = a * x - (x ⋅ x) * x\nfunction myfun!(outputs::Vector, inputs::Vector)\n    x0 = ones(Vec{2}) # Initial guess\n    a = inputs[1] + 2 * inputs[2]\n    x, converged = ad_newtonsolve(rf, x0, (a,))\n    outputs[1] = x ⋅ x\n    outputs[2] = a * x[1]\n    return outputs \nend\nout = zeros(2); inp = [1.2, 0.5]\nForwardDiff.jacobian(myfun!, out, inp)\n\ngives\n\n2×2 Matrix{Float64}:\n 1.0      2.0\n 1.57321  3.14643\n\nnote: Note\nThe maximum length of rf_dual_args where it is highly efficient is currently 5. For longer length there will be a dynamic dispatch, but this number can be extended  by adding more methods to the internal Newton.get_dual_results function.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"This approach is faster then naively differentiating a call which includes a newtonsolve, as we avoid iterating using Dual numbers. ","category":"page"},{"location":"api/","page":"API","title":"API","text":"using Newton, Tensors, ForwardDiff, BenchmarkTools\nrf(x::Vec, a::Number) = a * x - (x ⋅ x) * x\nfunction myfun!(outputs::Vector, inputs::Vector)\n    x0 = ones(Vec{2}) # Initial guess\n    a = inputs[1] + 2 * inputs[2]\n    x, converged = ad_newtonsolve(rf, x0, (a,))\n    outputs[1] = x ⋅ x\n    outputs[2] = a * x[1]\n    return outputs \nend\nfunction myfun2!(outputs::Vector, inputs::Vector)\n    x0 = ones(Vec{2}) # Initial guess\n    a = inputs[1] + 2 * inputs[2]\n    x, _, converged = newtonsolve(x -> rf(x, a), x0)\n    outputs[1] = x ⋅ x\n    outputs[2] = a * x[1]\n    return outputs\nend\nJ = zeros(2,2)\nout = zeros(2); inp = [1.2, 0.5]\ncfg = ForwardDiff.JacobianConfig(myfun!, out, inp)\ncfg2 = ForwardDiff.JacobianConfig(myfun2!, out, inp)\n# Call the standard function using newtonsolve\n@btime myfun2!($out, $inp);                                     # 143.381 ns (0 allocations: 0 bytes)\n# Differentiate through newtonsolve\n@btime ForwardDiff.jacobian!($J, $myfun2!, $out, $inp, $cfg2);  # 285.662 ns (0 allocations: 0 bytes)\n# Call the function which uses ad_newtonsolve (no difference)\n@btime myfun!($out, $inp);                                      # 143.381 ns (0 allocations: 0 bytes)\n# Differentiate through ad_newtonsolve\n@btime ForwardDiff.jacobian!($J, $myfun!, $out, $inp, $cfg);    # 183.359 ns (0 allocations: 0 bytes)","category":"page"},{"location":"api/","page":"API","title":"API","text":"showing that we get quite close to a regular non-differentiating call wrt. computational time in this microbenchmark.","category":"page"},{"location":"api/#Internal-API","page":"API","title":"Internal API","text":"","category":"section"},{"location":"api/#Newton.linsolve!","page":"API","title":"Newton.linsolve!","text":"linsolve!(K::AbstractMatrix, b::AbstractVector, cache::NewtonCache)\n\nSolves the linear equation system Kx=b, mutating both K and b. b is mutated to the solution x.\n\nlinsolve!(linsolver, K::AbstractMatrix, b::AbstractVector, cache::NewtonCache)\n\nThe default implementation will call this signature, which should be overloaded for a different  linsolver passed to cache upon construction.\n\n\n\n\n\n","category":"function"},{"location":"api/#Newton.linsolve","page":"API","title":"Newton.linsolve","text":"linsolve(linsolver, K, b)\n\nSolve the equation (system) b = K ⋆ x where ⋆ is the appropriate contraction between K and x, and b and x have the same type and size. The following combinations of K and x are supported. \n\nK b, x\nNumber Number\nSMatrix{N, N} SVector{N}\nSecondOrderTensor{d} AbstractTensor{o, d}\nFourthOrderTensor{d} AbstractTensor{o, d}\n\n\n\n\n\n","category":"function"},{"location":"api/#Newton.extract_submatrix","page":"API","title":"Newton.extract_submatrix","text":"extract_submatrix(::Type{SMatrix{d1, d2}}, m::SMatrix, start_row, start_col)\n\nEfficiently extract s::SMatrix{d1, d2} such that  s == m[start_row:(start_row + d1 - 1), start_col:(start_col + d2 - 1)]\n\n\n\n\n\n","category":"function"},{"location":"api/#Newton.join_submatrices","page":"API","title":"Newton.join_submatrices","text":"join_submatrices(a11, a12, a21, a22)\n\nEfficiently join the submatrices to return a::SMatrix = [a11 a12; a21 a22].\n\n\n\n\n\n","category":"function"},{"location":"#Newton","page":"Home","title":"Newton","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Newton.jl provides a fast and efficient newton-raphson  solver that is suitable to be used inside a preformance critical loop. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"ForwardDiff is used for the differentiation.\nRecursiveFactorization is used for LU-factorization of regular matrices\nStaticArrays.jl and Tensors.jl are also supported","category":"page"},{"location":"","page":"Home","title":"Home","text":"A logging mode can be enabled, see Newton.logging_mode.  When more fine-grained controlled, different algorithms etc. is desired,  consider NonlinearSolve.jl. ","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/KnutAM/Newton.jl\")\nusing Newton","category":"page"},{"location":"#Typical-usage","page":"Home","title":"Typical usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Solve r(x)=0 by calling newtonsolve","category":"page"},{"location":"","page":"Home","title":"Home","text":"x, drdx, converged = newtonsolve(rf!::Function, x::Vector, cache)\nx, drdx, converged = newtonsolve(rf::Function,  x::Union{Real, SVector, AbstractTensor})","category":"page"},{"location":"#Usage-inside-automatically-differentiated-functions","page":"Home","title":"Usage inside automatically differentiated functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If used in a function which should be differentiated by using ForwardDiff.jl, Tensors.jl, or any  other framework that uses ForwardDiff.jl's Dual number type, solve r(x, dual_args...) = 0 where the elements in dual_args are, or contain, numbers of Dual type  (e.g. Dual, Vector{<:Dual} or AbstractTensor{order, dim, <:Dual}, SVector{N, <:Dual}), call ad_newtonsolve","category":"page"},{"location":"","page":"Home","title":"Home","text":"x, converged = ad_newtonsolve(rf::Function,  x::Union{Real, SVector, AbstractTensor}, (dual_args...,))","category":"page"},{"location":"","page":"Home","title":"Home","text":"which will return x as a Dual value, or containing Dual values reflecting the derivative of x considering that  rf(x, dual_args...) = 0, such that dr/dx = 0 where x is a function of dual_args. ","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"#Mutating-(standard)-Array","page":"Home","title":"Mutating (standard) Array","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Initial setup (before running simulation):  Define a mutating residual function rf! which depends on  parameters, e.g. a and b, only available during the simulation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"function rf!(r::Vector, x::Vector, a, b)\n    return map!(v->(exp(a*v)-b*v^2), r, x)\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"Define the array x for the unknowns that will later be used, and preallocate cache","category":"page"},{"location":"","page":"Home","title":"Home","text":"x=zeros(5)\ncache = NewtonCache(x)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Runtime setup (inside simulation): At the place where we want to solve the problem r(x)=0","category":"page"},{"location":"","page":"Home","title":"Home","text":"a, b = rand(2); # However they are calculated during simulations\nrf_closure!(r_, x_) = rf!(r_, x_, a, b)\nx0 = getx(cache)\n# Modify x0 as you wish to provide initial guess\nx, drdx, converged = newtonsolve(rf_closure!, x0, cache)","category":"page"},{"location":"","page":"Home","title":"Home","text":"It is not necessary to get x0 from the cache, but this avoids allocating it. However, this implies that x0 will be aliased to the output, i.e. x0===x after solving. ","category":"page"},{"location":"#Non-mutating-StaticArray","page":"Home","title":"Non-mutating StaticArray","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Initial setup (before running simulation):  When using static arrays, the residual function should be non-mutating, i.e. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"function rf(x::SVector, a, b)\n    return exp.(a*x) - b*x.^2\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"Runtime setup (inside simulation): At the place where we want to solve the problem r(x)=0 No cache setup is required for static arrays. Hence, get the inputs a and b, define the true residual function with signature r=rf(x), define an initial guess x0, and call the newtonsolve","category":"page"},{"location":"","page":"Home","title":"Home","text":"a=rand(); b=rand();\nrf_closure(x_) = rf(x_, a, b)\nx0 = zero(SVector{5})\nx, drdx, converged = newtonsolve(rf_closure, x0);","category":"page"},{"location":"","page":"Home","title":"Home","text":"which as in the mutatable array case returns a the solution vector, the jacobian at the solution and a boolean whether  the solver converged or not. ","category":"page"},{"location":"#Benchmarks","page":"Home","title":"Benchmarks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See benchmarks/benchmark.jl, on my laptop the results are","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> activate benchmarks/\njulia> include(\"benchmarks/benchmarks.jl\");\nBenchmark with dim=5\nrf (static):           33.099 ns (0 allocations: 0 bytes)\nrf (dynamic):          32.931 ns (0 allocations: 0 bytes)\nnewtonsolve static:    1.000 μs (0 allocations: 0 bytes)\nnewtonsolve dynamic:   2.400 μs (11 allocations: 1.50 KiB)\nnlsolve dynamic:       6.900 μs (58 allocations: 6.23 KiB)\n\nBenchmark with dim=10\nrf (static):           61.491 ns (0 allocations: 0 bytes)\nrf (dynamic):          66.187 ns (0 allocations: 0 bytes)\nnewtonsolve static:    4.200 μs (0 allocations: 0 bytes)\nnewtonsolve dynamic:   5.100 μs (7 allocations: 5.28 KiB)\nnlsolve dynamic:       11.400 μs (58 allocations: 12.25 KiB)\n\nBenchmark with dim=20\nrf (static):           119.333 ns (0 allocations: 0 bytes)\nrf (dynamic):          125.471 ns (0 allocations: 0 bytes)\nnewtonsolve static:    7.900 μs (16 allocations: 14.81 KiB)\nnewtonsolve dynamic:   14.600 μs (5 allocations: 4.38 KiB)\nnlsolve dynamic:       29.100 μs (62 allocations: 23.39 KiB)\n\nBenchmark with dim=40\nrf (static):           265.634 ns (0 allocations: 0 bytes)\nrf (dynamic):          251.370 ns (0 allocations: 0 bytes)\nnewtonsolve static:    38.600 μs (16 allocations: 53.69 KiB)\nnewtonsolve dynamic:   53.200 μs (5 allocations: 4.38 KiB)\nnlsolve dynamic:       83.400 μs (67 allocations: 55.67 KiB)","category":"page"},{"location":"","page":"Home","title":"Home","text":"showing that static arrays are faster than dynamic arrays with newtonsolve and that newtonsolve outperforms nlsolve in these specific cases. (nlsolve does not  support StaticArrays.)","category":"page"}]
}
